# -- coding: utf-8 --
"""prediction_logic.py

Automatically generated by Colab style.
This module trains linear regression models per asset when run directly,
and exposes prediction helpers for backend to import.
"""

import os
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import pickle
import json
from typing import Dict, Any

# -------------------------------
# 1Ô∏è‚É£ Load & preprocess dataset
# -------------------------------
# Ensure the file path is correct for your environment.
# Use relative path if the Excel file sits next to this script or update to absolute path.
file_path = "histretSP45783b8.xls"  # <-- change to e.g. r"C:\path\to\histretSP45783b8.xls" on Windows if needed

# Safe load function
def load_dataset(path: str) -> pd.DataFrame:
    if not os.path.exists(path):
        raise FileNotFoundError(f"Dataset not found at: {path}. Please place the file or update file_path.")
    df_local = pd.read_excel(path, sheet_name="Returns by year", header=19)
    df_local = df_local.dropna(axis=1, how='all')
    df_local.columns = [col.strip() if isinstance(col, str) else f"Col_{i}" for i, col in enumerate(df_local.columns)]
    return df_local

# Try to load dataset (when module is imported this won't crash the app,
# because we only call load_dataset inside functions or under _main_)
try:
    _df_preview = None
    if os.path.exists(file_path):
        _df_preview = pd.read_excel(file_path, sheet_name="Returns by year", header=19, nrows=1)
except Exception:
    # ignore on import; detailed load happens later
    _df_preview = None

# -------------------------------
# Helper: feature engineering
# -------------------------------
def prepare_features(df: pd.DataFrame) -> pd.DataFrame:
    asset_cols = [
        "S&P 500 (includes dividends)", "US Small cap (bottom decile)", "3-month T.Bill",
        "US T. Bond (10-year)", "Baa Corporate Bond", "Real Estate", "Gold*"
    ]
    macro_cols = ["Inflation Rate", "Historical ERP"]

    # Convert to numeric safely
    for col in asset_cols + macro_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    df = df.dropna(subset=asset_cols + macro_cols).reset_index(drop=True)

    # Feature engineering: lag 1-3, rolling mean/std, momentum
    for col in asset_cols + macro_cols:
        for lag in range(1, 4):
            df[f'{col}_lag{lag}'] = df[col].shift(lag)
    for col in asset_cols:
        df[f'{col}_roll_mean'] = df[col].shift(1).rolling(window=3).mean()
        df[f'{col}_roll_std'] = df[col].shift(1).rolling(window=3).std()
        df[f'{col}_momentum'] = df[f'{col}_lag1'] - df[f'{col}_lag2']

    # macro interaction
    df['Inflation_ERP_interaction'] = df['Inflation Rate'].shift(1) * df['Historical ERP'].shift(1)
    df = df.dropna().reset_index(drop=True)
    return df

# -------------------------------
# Training helper (used when running file directly)
# -------------------------------
def train_and_save_models(df: pd.DataFrame, save_models_path="linear_asset_models.pkl",
                          save_feature_cols_path="linear_feature_columns.pkl",
                          save_latest_features_json="latest_features.json"):
    asset_cols = [
        "S&P 500 (includes dividends)", "US Small cap (bottom decile)", "3-month T.Bill",
        "US T. Bond (10-year)", "Baa Corporate Bond", "Real Estate", "Gold*"
    ]
    macro_cols = ["Inflation Rate", "Historical ERP"]

    df_prepared = prepare_features(df)

    models = {}
    feature_cols_dict = {}

    for col in asset_cols:
        features = (
            [f'{col}_lag{i}' for i in range(1, 4)] +
            [f'{m}_lag{i}' for m in macro_cols for i in range(1, 4)] +
            [f'{col}_roll_mean', f'{col}_roll_std', f'{col}_momentum'] +
            ['Inflation_ERP_interaction']
        )
        X = df_prepared[features].astype(float)
        y = df_prepared[col].astype(float)
        model = LinearRegression()
        model.fit(X, y)
        models[col] = model
        feature_cols_dict[col] = features

    # Save artifacts
    with open(save_models_path, "wb") as f:
        pickle.dump(models, f)
    with open(save_feature_cols_path, "wb") as f:
        pickle.dump(feature_cols_dict, f)

    latest_features_dict = df_prepared.iloc[-1].to_dict()
    with open(save_latest_features_json, "w") as f:
        json.dump(latest_features_dict, f)

    return models, feature_cols_dict, latest_features_dict

# -------------------------------
# Risk profile allocation (example)
# -------------------------------
risk_allocation = {
    "low": {
        "S&P 500 (includes dividends)": 0.15,
        "US Small cap (bottom decile)": 0.05,
        "3-month T.Bill": 0.25,
        "US T. Bond (10-year)": 0.25,
        "Baa Corporate Bond": 0.15,
        "Real Estate": 0.10,
        "Gold*": 0.05
    },
    "moderate": {
        "S&P 500 (includes dividends)": 0.30,
        "US Small cap (bottom decile)": 0.10,
        "3-month T.Bill": 0.10,
        "US T. Bond (10-year)": 0.15,
        "Baa Corporate Bond": 0.10,
        "Real Estate": 0.15,
        "Gold*": 0.10
    },
    "high": {
        "S&P 500 (includes dividends)": 0.40,
        "US Small cap (bottom decile)": 0.20,
        "3-month T.Bill": 0.05,
        "US T. Bond (10-year)": 0.10,
        "Baa Corporate Bond": 0.05,
        "Real Estate": 0.10,
        "Gold*": 0.10
    }
}

# -------------------------------
# 4Ô∏è‚É£ Define the main prediction function (keeps signature similar to your working Colab version)
# -------------------------------
def predict_portfolio(user_input: dict,
                      models: Dict[str, Any],
                      feature_cols_dict: Dict[str, Any],
                      latest_features_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    user_input: user JSON (amount, tenure, risk_profile, optional rebalancing_prefs)
    models: dict of trained models (per asset)
    feature_cols_dict: dict mapping asset -> list of feature names used during training
    latest_features_data: dict of latest feature values (a row from prepared df as dict)
    Returns: output JSON (cleaned: no per-asset allocation/explanation unless needed)
    """
    amount = user_input.get("amount", 100000)
    tenure = user_input.get("tenure", 5)
    risk_profile = user_input.get("risk_profile", "moderate")
    rebalancing_prefs = user_input.get("rebalance_preferences", {})

    allocation = risk_allocation[risk_profile.lower()]
    portfolio_expected_return = 0.0

    # Use a Series/DF to preserve feature names when predicting (avoids sklearn warnings)
    latest_features = pd.Series(latest_features_data)

    # per_asset internal (kept minimal)
    per_asset = {}
    for asset, weight in allocation.items():
        features = feature_cols_dict[asset]
        # Build a single-row DataFrame with column names the model expects
        X_latest_df = pd.DataFrame([ {f: latest_features[f] for f in features} ], columns=features)
        pred = models[asset].predict(X_latest_df)[0]
        pred_adj = pred * (1 + 0.02 * (tenure - 1))  # simple tenure heuristic
        per_asset[asset] = {"predicted_return": float(pred_adj), "weight": weight}
        portfolio_expected_return += pred_adj * weight

    portfolio_value = amount * (1 + portfolio_expected_return)

    # -------------------------------
    #  üìä Yearly Projection Simulation (compounds annually)
    # -------------------------------
    yearly_projection = []
    current_values = {asset: amount * weight for asset, weight in allocation.items()}

    for year in range(1, int(tenure) + 1):
        total_value = 0.0
        # for this year, apply per-asset predicted_return
        # NOTE: no intermediate rebalancing applied here (rebalancing plan provided separately)
        for asset, value in list(current_values.items()):
            r = per_asset[asset]["predicted_return"]
            new_value = value * (1 + r)
            current_values[asset] = new_value
            total_value += new_value

        # Record only the totals in output (sir asked to remove per-asset details in final JSON)
        yearly_projection.append({
            "year": year,
            "total_portfolio_value": round(total_value, 2)
        })

    # -------------------------------
    #  üîÅ Rebalancing Plan (rule-based)
    # -------------------------------
    rebalancing_plan = []
    if rebalancing_prefs:
        freq = rebalancing_prefs.get("frequency_years", 2)
        eq_decrease = rebalancing_prefs.get("equity_decrease_percent", 5)
        years = int(tenure // freq)
        temp_allocation = allocation.copy()
        for i in range(1, years + 1):
            for eq_asset in ["S&P 500 (includes dividends)", "US Small cap (bottom decile)"]:
                temp_allocation[eq_asset] = max(temp_allocation[eq_asset] - eq_decrease / 100, 0)
            rebalancing_plan.append({
                "year": i * freq,
                "action": f"Reduce equity weight by {eq_decrease}%",
                "new_allocation": temp_allocation.copy()
            })

    # -------------------------------
    #  üí° User Guidance
    # -------------------------------
    guidance = [
        f"Expected portfolio return: {portfolio_expected_return:.3f}",
        "Equity assets contribute most to returns for moderate/high risk profiles.",
        "Bonds and T-Bills stabilize portfolio in low/moderate risk profiles."
    ]

    # -------------------------------
    #  ‚úÖ Clean Output JSON (no per-asset allocation/explanations; only summary & yearly totals)
    # -------------------------------
    output_json = {
        "input_summary": user_input,
        "portfolio_allocation": allocation,
        "portfolio_summary": {
            "expected_portfolio_return": portfolio_expected_return,
            "expected_portfolio_value": portfolio_value,
            "top_drivers": sorted(allocation.keys(),
                                  key=lambda x: per_asset[x]['predicted_return'] * allocation[x],
                                  reverse=True)[:3],
            "risk_level": risk_profile
        },
        "yearly_projection": yearly_projection,
        "rebalancing_plan": rebalancing_plan,
        "user_guidance": guidance
    }

    return output_json

# -------------------------------
# Convenience wrapper for backend: load artifacts from disk and predict
# -------------------------------
def predict_portfolio_from_disk(user_input: dict,
                                models_path: str = "linear_asset_models.pkl",
                                feature_cols_path: str = "linear_feature_columns.pkl",
                                latest_features_path: str = "latest_features.json") -> Dict[str, Any]:
    if not os.path.exists(models_path) or not os.path.exists(feature_cols_path) or not os.path.exists(latest_features_path):
        raise FileNotFoundError("Models or feature definitions or latest features are missing. "
                                "Run prediction_logic.py as a script to generate artifacts, or place the files in the working dir.")
    with open(models_path, "rb") as f:
        models = pickle.load(f)
    with open(feature_cols_path, "rb") as f:
        feature_cols_dict = pickle.load(f)
    with open(latest_features_path, "r") as f:
        latest_features_dict = json.load(f)

    return predict_portfolio(user_input, models, feature_cols_dict, latest_features_dict)

# -------------------------------
# When run directly: train and dump artifacts (keeps import-time light)
# -------------------------------
if __name__ == "__main__":
    try:
        print("Preparing dataset and training models (this runs only when script is executed directly)...")
        df_raw = load_dataset(file_path)
        models_out, feature_cols_out, latest_features_out = train_and_save_models(df_raw)
        print("Saved models to linear_asset_models.pkl, feature columns to linear_feature_columns.pkl, latest_features.json.")
    except Exception as e:
        print("Error while running as script:", str(e))
        raise